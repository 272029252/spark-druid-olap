#+TITLE:    TPCH Benchmark
#+AUTHOR:    Harish Butani
#+EMAIL:     hbutani@apache.org
#+LANGUAGE:  en
#+INFOJS_OPT: view:showall toc:t ltoc:t mouse:underline path:http://orgmode.org/org-info.js
#+LINK_HOME: http://home.fnal.gov/~neilsen
#+LINK_UP: http://home.fnal.gov/~neilsen/notebook
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://orgmode.org/org-manual.css" />

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [pdftex,10pt,a4paper]

#+LaTeX_HEADER: \usepackage{sectsty}
#+LaTeX_HEADER: \usepackage{fancyvrb}

#+LaTeX_HEADER: \usepackage{hyperref}
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xyling}
#+LaTeX_HEADER: \usepackage{ctable}
#+LaTeX_HEADER: \usepackage{url}

#+LaTeX_HEADER: \input xy
#+LaTeX_HEADER: \xyoption{all}

#+LaTeX_HEADER: \usepackage[backend=bibtex,sorting=none]{biblatex}
#+LaTeX_HEADER: \addbibresource{SparkDruid.bib}

#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+OPTIONS: H:4 num:nil toc:nil \n:nil @:t ::t |:t ^:{} _:{} *:t TeX:t LaTeX:t
#+STARTUP: showall
#+OPTIONS: html-postamble:nil

We ran a benchmark on a set of representative queries that contrast performance
 of queries being rewritten to use a DruidIndex vs.
running the Queries directly against the *raw event* DataSet. We have
attempted to make this test as fair as possible: *by not taking any advantages of preaggregations and column pruning in Druid, and by using in
memory caching when the queries run only in Spark*. We ran the 2
scenarios on the same cluster: for the Druid run we gave the cluster
node resources to Druid History servers, for the Spark run we ran
Spark Executors on the worker nodes.

For the benchmark we used the [[http://www.tpc.org/tpch/spec/tpch2.8.0.pdf][TPCH benchmark dataset]], datascale
10G. We converted the 10G star schema into a flattened(denormalized)
transaction dataset. For Spark we further processed the data to setup
a Partitioned table,  stored in Parquet format; the table is
partitioned by month. The Druid 
Index was created using the [[http://druid.io/docs/latest/ingestion/batch-ingestion.html][HadoopDruidIndexer]]. 

The dataset sizes are:

| TPCH Flat TSV                        | 46.80GB |
| Druid Index in HDFS                  | 17.04GB |
| TPCH Flat Parquet                    | 11.38GB |
| TPCH Flat Parquet Partition by Month | 11.56GB |

* Queries

We chose the queries to test the performance differences for /time
intervals/, /dimension filters/, /time slice aggregation/, and
/dimensional aggregations/. We have tested for TPCH queries Q1, Q3,
Q5, Q7 and Q8. These queries have been altered so that they can be
rewritten as Druid queries. As explained below, these don't change
overall benchmark conclusion, if anything the numbers will skew
further in favor of Druid once we start to push Sorts and Limits to
Druid.  


*Alterations made to queries:*
- Formulas in aggregations have been removed. For
  e.g. =sum(l_extendedprice*(1-l_discount))= is simply written as
  =sum(extendedprice)=. This is because we haven't implemented the rewrite to handle
  aggregation formulas. This has a minor impact on query performance;
  so the overall benchmark analysis will not change.
- Since we haven't implemented the rewrites, we have removed *order
  by, limit* clauses from the queries. Again this shouldn't change the
  overall benchmark conclusion; in fact once we can push down order
  and limit to Druid, the rewritten query times will reduce.

We have summarized the Query forms below. [[Appendix A: Query Details]]
describes the Queries in detail.

** Query Summary:
#+attr_latex: align=|l|l|l|l|l|
|-----------------+-----------------+------------+---------------+----------------|
| *Query*         | *Interval*      | *Filters*  | *Group By*    | *Aggregations* |
|-----------------+-----------------+------------+---------------+----------------|
| Basic           | None            | None       | ReturnFlag    | Count(*)       |
| Aggregation.    |                 |            | LineStatus    | Sum(exdPrice)  |
|                 |                 |            |               | Avg(avlQty)    |
|-----------------+-----------------+------------+---------------+----------------|
| Ship Date Range | 1995-12/1997-09 | None       | ReturnFlag    | Count(*)       |
|                 |                 |            | LineStatus    |                |
|-----------------+-----------------+------------+---------------+----------------|
| SubQry          |                 |            | S_Nation      |                |
| nation, pType   |                 |            |               |                |
| ShpDt Range     | 1995-12/1997-09 | P_Type     |               | Count(*)       |
|                 |                 | S_Nation + |               | Sum(exdPrice)  |
|                 |                 | C_Nation   |               | Max(sCost)     |
|                 |                 |            |               | Avg(avlQty)    |
|                 |                 |            |               | Count(         |
|                 |                 |            |               | Distinct oKey) |
|-----------------+-----------------+------------+---------------+----------------|
| TPCH Q1         | None            | None       | ReturnFlag    | Count(*)       |
|                 |                 |            | LineStatus    | Sum(exdPrice)  |
|                 |                 |            |               | Max(sCost)     |
|                 |                 |            |               | Avg(avlQty)    |
|                 |                 |            |               | Count(         |
|                 |                 |            |               | Distinct oKey) |
|-----------------+-----------------+------------+---------------+----------------|
| TPCH Q3         | 1995-03-15-     | O_Date     | OKey          | Sum(exdPrice)  |
|                 |                 | MktSegment | ODate         |                |
|                 |                 |            | ShipPri       |                |
|-----------------+-----------------+------------+---------------+----------------|
| TPCH Q5         | None            | O_Date     | S_Nation      | Sum(exdPrice)  |
|                 |                 | Region     |               |                |
|-----------------+-----------------+------------+---------------+----------------|
| TPCH Q7         | None            | S_Nation + | S_Nation      | Sum(exdPrice)  |
|                 |                 | C_Nation   | C_Nation      |                |
|                 |                 |            | ShipDate.Year |                |
|-----------------+-----------------+------------+---------------+----------------|
| TPCH Q8         | None            | Region     | ODate.Year    | Sum(exdPrice)  |
|                 |                 | Type       |               |                |
|                 |                 | O_Date     |               |                |
|-----------------+-----------------+------------+---------------+----------------|

The Queries are described in detail in the  [[Appendix A: Query Details][Query Details]] section.
  

* Benchmark Results

[[Appendix B: Benchmark Environment]] and
 [[Appendix C: Running the Benchmark]] describe in detail where and how
 the Benchmark was run. In this section we present present our findings.

** TPCH 10G dataset
*Running against raw event Dataset:*


| Query                            |  Avg. Time | Min. Time | Max. Time |
|----------------------------------+------------+-----------+-----------|
| Basic Aggregation                | 273371.000 |     78884 |    681879 |
| Ship Date Range                  | 221933.000 |     65074 |    503307 |
| SubQuery + filters + ShpDt Range |  86999.000 |     13762 |    417972 |
| TPCH Q1                          | 183568.000 |     61652 |    380407 |
| TPCH Q3                          | 270054.000 |     30141 |    522393 |
| TPCH Q5                          | 172155.000 |     50255 |    292185 |
| TPCH Q7                          |  70663.000 |     17134 |    300033 |
| TPCH Q8                          |  19823.000 |     12287 |     38247 |
|                                  |            |           |           |

*Running against Druid:*

| Query                            | Avg. Time | Min. Time | Max. Time |
|----------------------------------+-----------+-----------+-----------|
| Basic Aggregation                | 20324.000 |     19873 |     20776 |
| Ship Date Range                  |  1768.000 |      1712 |      1824 |
| SubQuery + filters + ShpDt Range |   244.000 |       195 |       294 |
| TPCH Q1                          | 18340.000 |     17783 |     18897 |
| TPCH Q3                          | 10669.000 |     10345 |     10994 |
| TPCH Q5                          | 16722.000 |     16617 |     16828 |
| TPCH Q7                          |   862.000 |       712 |      1012 |
| TPCH Q8                          | 20429.000 |     20190 |     20669 |

** Observations about the performance:
- The /SubQuery + filters + ShpDt Range/  query gives the most benefit
  when rewritten to Druid. This is not surprising, as this Query is
  tailor-made for Druid. The *Interval* and *Dimension* predicates
  fully leverage the Segment pruning and inverted index layout of
  Druid. 
- The /Ship Date Range/ query also shows a significant boost when run
  on Druid. This is surprising since Spark partition pruning should
  have nullified the segment pruning Druid will do. We ensure that
  Spark is doing partition pruning, by explicitly adding /shipYear/
  predicates to Spark. Is this attributable to aggregations being
  quicker in Druid, or is this the overhead of predicate evaluation
  that Spark has to do for each row? Another aspect maybe that we didn't enable the [[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html][Code Generation]] option for
  Catalyst expressions, doing so may bridge this gap.
- /TPCH Q7/ shows a significant performance boost when running on Druid:
  milliSeconds vs. 10s of seconds. The difference is in applying the
  filter to each row when running on Spark vs. using the inverted
  index to quickly find what values to aggregate. The *year* level
  aggregation should have the same optimizations in play: Map side
  aggregation in the case of Spark, should significantly reduce the
  amount of rows being shuffled; and in the case of Druid the amount of data shipped to
  the Broker is small, because only year level aggregations are being
  shipped. *So queries with just dimensional predicates, and small
  output cardinality also get a significant boost when rewritten as Druid queries*.  
- For /TPCH Q3/, /TPCH Q5/ and /TPCH Q8/ there is an improvement, but nowhere close to the boost
  for *Q7*. *This is because the OrderDate predicate is translated to
  a JavascriptFilter*. We need to look into ways of translating such
  predicates to  a native java function.
- Queries /Basic Agg./ and /TPCH Q1/ show some improvement. The
  Count-Distinct is translated to a [[http://druid.io/docs/0.8.0/querying/aggregations.html][Cardinality Aggregator]] which is an
  approximate count. This is definitely an advantage for Druid, for
  large cardinality dimensions. We need to test against Spark using
  the HyperLogLog aggregator.

We have tried to make the comparisons as fair as possible: by indexing
at the lowest grain, and making the Druid index contain all
columns. We also partitioned and cached the Spark DataFrame. Inspite
of these steps, we find that a *Basic Aggregation* computation( even
with the Count-Distinct removed) is faster in Druid. Part of the issue
is that Spark needs more memory and compute resources then Druid; several Query
runs had failures, causing Executors to crash and hence tasks to be
redone. But the differences are not all explained by this, even if we
consider minimum times from the Spark runs, Druid still performs
better. They could be explained by the fact that we didn't run with 
 [[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html][Code Generation]] enabled. We plan to run a followup test with [[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html][Code Generation]]
turned on. 
We expect that for /Basic Agg./ and /TPCH Q1/  Spark to
perform as well as Druid. But for Queries involving *Dimensional* and
*Interval* slices, Druid is still an order of magnitude faster(a
seconds vs. 10 seconds). Building native functions in Druid will
expand the queries that gain this kind of performance boost. Queries
that have *Interval* and *Dimensional* predicates are very common in
OLAP, as analysts browse the Cube applying filters, Drilling Down
and Up.


** TPCH Slice dataset

*Running against raw event Dataset:*

| Query                            | Avg. Time | Min. Time | Max. Time |
|----------------------------------+-----------+-----------+-----------|
| Basic Aggregation                | 26890.000 |      7165 |    113328 |
| Ship Date Range                  | 10530.000 |      8499 |     16380 |
| SubQuery + filters + ShpDt Range |  4544.000 |      3626 |      7625 |
| TPCH Q1                          |  7993.000 |      7186 |     10483 |
| TPCH Q3                          |  6004.000 |      3727 |     11246 |
| TPCH Q5                          |  6614.000 |      5481 |      8979 |
| TPCH Q7                          |  5648.000 |      4524 |      7133 |
| TPCH Q8                          |  3804.000 |      3211 |      4780 |
| TPCH Q10                         | 27150.000 |     23269 |     35494 |
|                                  |           |           |           |



*Running against Druid:*

| Query                            | Avg. Time | Min. Time | Max. Time |
|----------------------------------+-----------+-----------+-----------|
| Basic Aggregation                |  3138.000 |      2327 |      4142 |
| Ship Date Range                  |   633.000 |       474 |       999 |
| SubQuery + filters + ShpDt Range |   284.000 |       151 |       828 |
| TPCH Q1                          |  2222.000 |      2005 |      2429 |
| TPCH Q3                          |  2066.000 |       910 |      7176 |
| TPCH Q5                          |  4514.000 |      4129 |      5501 |
| TPCH Q7                          |   538.000 |       318 |      1282 |
| TPCH Q8                          |  4922.000 |      4541 |      5424 |
| TPCH Q10                         | 13765.000 |     12808 |     17560 |
|                                  |           |           |           |


* Appendix A: Query Details
** Query Basic Aggregation
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT l_returnflag,
       l_linestatus,
       Count(*),
       Sum(l_extendedprice) AS s,
       Max(ps_supplycost)   AS m,
       Avg(ps_availqty)     AS a,
       Count(DISTINCT o_orderkey)
FROM   orderlineitempartsupplier
GROUP  BY l_returnflag,
          l_linestatus
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [l_returnflag#69,l_linestatus#70], [l_returnflag#69,l_linestatus#70,...
 Project [l_extendedprice#66,o_orderkey#53,ps_supplycost#81,l_returnflag#69,...
  Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,...
\end{lstlisting}
\end{tiny}

*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [l_returnflag#69,l_linestatus#70,alias-1#155L AS c2#109L,alias-2#154 AS s#106,...
 PhysicalRDD [alias-2#154,alias-3#158,alias-7#157,..], DruidRDD[6] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}

The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/basicAgg.json][here]]

** Query Ship Date Range
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]

SELECT f,
       s,
       Count(*) AS count_order
FROM   (SELECT l_returnflag AS f,
               l_linestatus AS s,
               l_shipdate,
               s_region,
               s_nation,
               c_nation
        FROM   orderlineitempartsupplier) t
WHERE  Dateisbeforeorequal(Datetime(`l_shipdate`),
              Dateminus(Datetime("1997-12-01"), Period("p90d")))
       AND Dateisafter(Datetime(`l_shipdate`), Datetime("1995-12-01"))
GROUP  BY f,
          s
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [f#127,s#128], [f#127,s#128,COUNT(1) AS count_order#120L]
 Project [l_returnflag#69 AS f#127,l_linestatus#70 AS s#128]
  Filter (scalaUDF(scalaUDF(l_shipdate#71),scalaUDF(scalaUDF(1997-12-01),scalaUDF(P90D)))...
   Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,...
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [f#127,s#128,alias-1#159L AS count_order#120L]
 PhysicalRDD [f#127,s#128,alias-1#159L], DruidRDD[7] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}


The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/shipDteRange.json][here]]
** Query SubQuery + nation,Type predicates + ShipDate Range
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT s_nation,
       Count(*)             AS count_order,
       Sum(l_extendedprice) AS s,
       Max(ps_supplycost)   AS m,
       Avg(ps_availqty)     AS a,
       Count(DISTINCT o_orderkey)
FROM   (SELECT l_returnflag AS f,
               l_linestatus AS s,
               l_shipdate,
               s_region,
               s_nation,
               c_nation,
               p_type,
               l_extendedprice,
               ps_supplycost,
               ps_availqty,
               o_orderkey
        FROM   orderlineitempartsupplier
        WHERE  p_type = 'ECONOMY ANODIZED STEEL') t
WHERE  Dateisbeforeorequal(Datetime(`l_shipdate`),
              Dateminus(Datetime("1997-12-01"), Period("p90d")))
       AND Dateisafter(Datetime(`l_shipdate`), Datetime("1995-12-01"))
       AND ( ( s_nation = 'FRANCE'
               AND c_nation = 'GERMANY' )
              OR ( c_nation = 'FRANCE'
                   AND s_nation = 'GERMANY' ) )
GROUP  BY s_nation
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [s_nation#88], [s_nation#88,COUNT(1) AS count_order#129L,SUM(l_extendedprice#66) AS ...
 Project [l_extendedprice#66,o_orderkey#53,ps_supplycost#81,s_nation#88,ps_availqty#80]
  Filter ((p_type#93 = ECONOMY ANODIZED STEEL) && ((scalaUDF(scalaUDF(l_shipdate#71),...
   Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,...
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [s_nation#88,alias-1#161L AS count_order#129L,alias-2#160 AS s#130,...
 PhysicalRDD [alias-2#160,alias-3#164,...], DruidRDD[8] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}


The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/projFiltRange.json][here]]
** Query TPCH Q1
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT l_returnflag,
       l_linestatus,
       Count(*),
       Sum(l_extendedprice) AS s,
       Max(ps_supplycost)   AS m,
       Avg(ps_availqty)     AS a,
       Count(DISTINCT o_orderkey)
FROM   orderlineitempartsupplier
GROUP  BY l_returnflag,
          l_linestatus
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [l_returnflag#69,l_linestatus#70], [l_returnflag#69,l_linestatus#70,COUNT(1) AS c2#145L,...
 Project [l_extendedprice#66,o_orderkey#53,ps_supplycost#81,l_returnflag#69,l_linestatus#70,...
  Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,....
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [l_returnflag#69,l_linestatus#70,alias-1#166L AS c2#145L,alias-2#165 AS s#142,...
 PhysicalRDD [alias-2#165,alias-3#169,...], DruidRDD[9] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}
The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/q1.json][here]]
** Query TPCH Q3 - extendePrice instead of revenue; order, limit removed
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT o_orderkey,
       Sum(l_extendedprice) AS price,
       o_orderdate,
       o_shippriority
FROM   orderlineitempartsupplier
WHERE  c_mktsegment = 'BUILDING'
       AND Dateisbefore(Datetime(`o_orderdate`), Datetime("1995-03-15"))
       AND Dateisafter(Datetime(`l_shipdate`), Datetime("1995-03-15"))
GROUP  BY o_orderkey,
          o_orderdate,
          o_shippriority
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [o_orderkey#53,o_orderdate#57,o_shippriority#60], [o_orderkey#53,...
 Project [o_orderkey#53,o_orderdate#57,o_shippriority#60,l_extendedprice#66]
  Filter (((c_mktsegment#102 = BUILDING) && scalaUDF(scalaUDF(o_orderdate#57),...
   Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,...
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [CAST(o_orderkey#53, IntegerType) AS o_orderkey#53,alias-1#170 AS ...
 PhysicalRDD [o_orderkey#53,...], DruidRDD[10] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}
The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/q3.json][here]]
** Query TPCH Q5 - extendePrice instead of revenue
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT s_nation,
       Sum(l_extendedprice) AS extendedPrice
FROM   orderlineitempartsupplier
WHERE  s_region = 'ASIA'
       AND Dateisafterorequal(Datetime(`o_orderdate`), Datetime("1994-01-01"))
       AND Dateisbefore(Datetime(`o_orderdate`),
           Dateplus(Datetime("1994-01-01"), Period(
               "p1y")))
GROUP  BY s_nation
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [s_nation#88], [s_nation#88,SUM(l_extendedprice#66) AS extendedPrice#148]
 Project [s_nation#88,l_extendedprice#66]
  Filter (((s_region#89 = ASIA) && scalaUDF(scalaUDF(o_orderdate#57),scalaUDF(1994-01-01))) ...
   Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,....
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [s_nation#88,alias-1#171 AS extendedPrice#148]
 PhysicalRDD [s_nation#88,alias-1#171], DruidRDD[11] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}
The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/q5.json][here]]

** Query TPCH Q7 - price instead of revenue
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT s_nation,
       c_nation,
       Year(Datetime( ` l_shipdate ` )) AS l_year,
       Sum(l_extendedprice)             AS extendedPrice
FROM   orderlineitempartsupplier
WHERE  ( ( s_nation = 'FRANCE'
           AND c_nation = 'GERMANY' )
          OR ( c_nation = 'FRANCE'
               AND s_nation = 'GERMANY' ) )
GROUP  BY s_nation,
          c_nation,
          Year(Datetime( ` l_shipdate ` ))
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [s_nation#88,c_nation#104,scalaUDF(scalaUDF(l_shipdate#71))], ...
 Project [s_nation#88,c_nation#104,l_shipdate#71,l_extendedprice#66]
  Filter (((s_nation#88 = FRANCE) && (c_nation#104 = GERMANY)) || ...
   Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,....
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [s_nation#88,c_nation#104,CAST(l_shipdate#172, IntegerType) AS...
 PhysicalRDD [s_nation#88,...], DruidRDD[12] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}
The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/q7.json][here]]

** Query TPCH Q8 - extendedPrice instead of market share
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
SELECT Year(Datetime(`o_orderdate`)) AS o_year,
       Sum(l_extendedprice)          AS price
FROM   orderlineitempartsupplier
WHERE  c_region = 'AMERICA'
       AND p_type = 'ECONOMY ANODIZED STEEL'
       AND Dateisafterorequal(Datetime(`o_orderdate`), Datetime("1995-01-01"))
       AND Dateisbeforeorequal(Datetime(`o_orderdate`), Datetime("1996-12-31"))
GROUP  BY Year(Datetime(`o_orderdate`))
\end{lstlisting}
\end{tiny}

*** Logical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Aggregate [scalaUDF(scalaUDF(o_orderdate#57))], [scalaUDF(scalaUDF(o_orderdate#57)) AS ...
 Project [o_orderdate#57,l_extendedprice#66]
  Filter ((((c_region#105 = AMERICA) && (p_type#93 = ECONOMY ANODIZED STEEL)) && ...
   Relation[o_orderkey#53,o_custkey#54,o_orderstatus#55,o_totalprice#56,o_orderdate#57,...
\end{lstlisting}
\end{tiny}
*** Physical Plan
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
   \begin{lstlisting}[frame=shadowbox, numbers=left]
Project [CAST(o_orderdate#174, IntegerType) AS o_year#151,alias-1#175 AS price#152]
 PhysicalRDD [o_orderdate#174,alias-1#175], DruidRDD[13] at RDD at DruidRDD.scala:34
\end{lstlisting}
\end{tiny}
The Druid Query for this query is [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/queries/q8.json][here]]


* Appendix B: Benchmark Environment
** Cluster Details
The Benchmark was run on a 4 node cluster. Each node is a 2 core,16GB
memory, 256GB hard drive machine running centos 6.4. The output of the
=lscpu= and =hdparm= are listed below:

\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=BASH, showspaces=false, showstringspaces=false}
  \label{mcDetails}
   \begin{lstlisting}[caption={Machine Details},frame=shadowbox, numbers=left]

lscpu

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                2
On-line CPU(s) list:   0,1
Thread(s) per core:    1
Core(s) per socket:    1
Socket(s):             2
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 42
Stepping:              1
CPU MHz:               1999.999
BogoMIPS:              3999.99
Virtualization:        VT-x
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              4096K
NUMA node0 CPU(s):     0,1

sudo hdparm -tT /dev/vdb

/dev/vdb:
 Timing cached reads:   12798 MB in  2.00 seconds = 6408.97 MB/sec
 Timing buffered disk reads: 540 MB in  3.00 seconds = 179.98 MB/sec

\end{lstlisting}
\end{tiny}

The machines are setup with [[http://hortonworks.com/hdp/whats-new/][HDP 2.3]] using  [[https://cwiki.apache.org/confluence/display/AMBARI/Quick+Start+Guide][Ambari]]. Also installed
[[http://static.druid.io/artifacts/releases/druid-0.8.0-bin.tar.gz][Druid 0.8]] on the machines. The cluster is configured to use Yarn; we
installed and setup  [[http://spark.apache.org/downloads.html][Spark 1.4.1]] to run using the Yarn Resource
Manager.



** TPCH Flattened Dataset, scale 10
For the benchmark we used the [[http://www.tpc.org/tpch/spec/tpch2.8.0.pdf][TPCH benchmark dataset]], datascale
10G. We converted the 10G star schema into a flattened(denormalized)
transaction dataset using a tool we wrote [[https://github.com/SparklineData/tpch-spark-druid/blob/master/tpchData/src/main/scala/org/sparklinedata/tpch/hadoop/TpchGenFlattenedData.scala][TpchGenFlattenedData]], for
example we ran it like this:
\begin{Verbatim}[frame=single]
spark/bin/spark-submit –num-executors 7 \
–properties-file spark-druid/spark.properties \
–packages com.databricks:spark-csv2.10:1.1.0 \
–jars spark-druid/spark-datetime-assembly-0.0.1.jar,\
      spark-druid/spark-druid-olap-assembly-0.0.1.jar \
–class org.sparklinedata.tpch.hadoop.TpchGenFlattenedData \
spark-druid/tpchdata-assembly-0.0.1.jar \
tpchflatorc10 tpchflattened
\end{Verbatim}

** Dataset for Spark Queries
For spark we further processed the data to setup a Partitioned table,
stored in Parquet format; the table is partitioned by month. We use the
[[https://github.com/SparklineData/tpch-spark-druid/blob/master/tpchData/src/main/scala/org/sparklinedata/tpch/hadoop/TpchBuildParquetPartitioned.scala][TpchBuildParquetPartitioned]] to do this. 

** Druid Index for TPCH Flattened Dataset
The Druid Index was created using the [[http://druid.io/docs/latest/ingestion/batch-ingestion.html][HadoopDruidIndexer]] with the
following command:
\begin{Verbatim}[frame=single]
java -Xmx256m -Dhdp.version=2.3.0.0-2557 -Duser.timezone=UTC \
-Dfile.encoding=UTF-8 -classpath \
$DIR/config/_common:$HADOOP_CONF_DIR:$DIR/lib/* \
io.druid.cli.Main index hadoop <spec_file>
\end{Verbatim}

See [[https://github.com/SparklineData/spark-druid-olap/blob/master/docs/benchmark/druid/tpch_index.json][Druid TPCH Index Specification]] for detailed specification of the
TPCH index in Druid. Key points of the Index:
- /l_shipdate/ is chosen as the time dimension. Based on the TPCH
  Query set, there is a significant number of queries that are time
  sliced based on the Ship Date.
- We indexed all the dimensions. The metrics are: ~o_totalprice,~
  ~l_quantity, l_extendedprice, ps_availqty, ps_supplycost,
  c_acctbal~. Rest of the columns are modeled as dimensions.
- *The index is created at the grain of raw events.*
- The Index time segment is chosen to be month.

Note by choosing to model all dimensions and by choosing to index at
the grain of events, we have made the Druid Index as big as
possible. *We are note giving Druid any advantages of preaggregation
or column pruning.* 

** DataSource setup

The raw event DataSource and Druid datasource are defined in the
following way:

\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=SQL, showspaces=false, showstringspaces=false}
  \label{rawEvntDS}
   \begin{lstlisting}[caption={Raw Event DataSource},frame=shadowbox, numbers=left]

// parquet based partitioned table
val df = sqlCtx.read.parquet(cfg.tpchFlatDir)
df.cache()
df.registerTempTable("orderLineItemPartSupplier")

// Druid Datasource
CREATE TEMPORARY TABLE orderLineItemPartSupplier
      USING org.sparklinedata.druid
      OPTIONS (sourceDataframe "$baseFlatTableName",
      timeDimensionColumn "l_shipdate",
      druidDatasource "tpch",
      druidHost "${cfg.druidBroker}",
      druidPort "8082");

\end{lstlisting}
\end{tiny}


* Appendix C: Running the Benchmark
*** Running against Druid Datasource
For the Druid Datasource experiment the queries are run on spark using the
[[https://github.com/SparklineData/spark-druid-olap/blob/master/src/main/scala/org/sparklinedata/druid/tools/TpchBenchMark.scala][Druid TpchBenchMark]] tool. It is run using the following command:
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=bash, showspaces=false, showstringspaces=false}
  \label{tpchDruidBmark}
   \begin{lstlisting}[caption={Running Tpchbenchmark on Druid Datasource},frame=shadowbox, numbers=left]

~/spark-1.4.1-bin-hadoop2.6/bin/spark-submit \
--properties-file spark.properties \
--packages com.databricks:spark-csv_2.10:1.1.0 \
--jars sparkjars/spark-datetime-assembly-0.0.1.jar  \
--class org.sparklinedata.druid.tools.TpchBenchMark \
sparkjars/spark-druid-olap-assembly-0.0.1.jar \
-n hb-1.openstacklocal \
-t tpchFlattenedData_10/orderLineItemPartSupplierCustomer \
-d hb-1.openstacklocal
\end{lstlisting}
\end{tiny}

The cluster is setup to run a historical server on each node. Each
historical server is configure with 8GB of memory:
\begin{Verbatim}[frame=single]
JAVA_HISTORICAL_OPTIONS="-server \
 -Xmx8g \
 -Xms8g \
 -XX:NewSize=1g \
 -XX:MaxNewSize=1g \
 -XX:MaxDirectMemorySize=10g \
 -XX:+UseConcMarkSweepGC \
 -XX:+PrintGCDetails \
 -XX:+PrintGCTimeStamps \
 -XX:+HeapDumpOnOutOfMemoryError \
 -Duser.timezone=UTC \
 -Dfile.encoding=UTF-8"
\end{Verbatim}

The spark shell is run in local mode on one of the nodes, so that
Spark uses as little cluster resources as possible.

*** Running against cached Spark DataFrame
We compare the rewritten queries against the case of not having a
Druid Index. In this case we try to give the Spark engine as much
advantage as we can. 

We give the Spark executors as much of the Yarn cluster
as possible. The Spark configuration is:
\begin{Verbatim}[frame=single]
spark.serializer=org.apache.spark.serializer.KryoSerializer
#spark.sql.autoBroadcastJoinThreshold=100000000
spark.sql.autoBroadcastJoinThreshold=-1
spark.sql.planner.externalSort=true

spark.executor.memory=9g
spark.driver.memory=2g
#spark.executor.cores=2
\end{Verbatim}

*As part of the initialization, the orderLineItemPartSupplier
DataFrame is cached in memory.*

For the queries going against Spark we used the
[[https://github.com/SparklineData/tpch-spark-druid/blob/master/tpchData/src/main/scala/org/sparklinedata/tpch/hadoop/TpchParquetBenchmark.scala][Spark TpchBenchmark]] tool. It is run with the following command:
\begin{tiny}
   \lstset{keywordstyle=\bfseries\underbar, emphstyle=\underbar,
     language=bash, showspaces=false, showstringspaces=false}
  \label{raweBmark}
   \begin{lstlisting}[caption={Running the Benchmark, on the Raw Event DataFrame},frame=shadowbox, numbers=left]

~/spark-1.4.1-bin-hadoop2.6/bin/spark-submit \
--properties-file spark.properties \
--packages com.databricks:spark-csv_2.10:1.1.0 \
--jars sparkjars/spark-datetime-assembly-0.0.1.jar,\
       sparkjars/spark-druid-olap-assembly-0.0.1.jar,\
       sparkjars/tpchdata-assembly-0.0.1.jar   \
--num-executors 4 --master yarn-client \
--class org.sparklinedata.tpch.hadoop.TpchParquetBenchmark \
sparkjars/tpchdata-assembly-0.0.1.jar \
-t tpchFlattenedData_10/\
orderLineItemPartSupplierCustomer.parquet.partitioned
\end{lstlisting}
\end{tiny}

* Appendix D: Query Results

** Raw Event Dataset Query Results
#+begin_example
Basic Aggregation
[A,F,1478160,5.653926919402009E10,1000.0,4997.955256535151,644914]
[R,F,1480195,5.6579322994170044E10,1000.0,4997.424184651347,645781]
[N,F,38767,1.4841356872899961E9,999.96,4990.055330564655,30750]
[N,O,2998603,1.1461753710263998E11,1000.0,4999.978290890792,768912]
Ship Date Range
[N,O,1599288]
SubQuery + nation,Type predicates + ShipDate Range
[FRANCE,16,687985.3700000001,933.5,5169.0,16]
[GERMANY,19,753095.0299999999,994.08,5400.421052631579,19]
TPCH Q1
[A,F,1478160,5.653926919402008E10,1000.0,4997.955256535151,644914]
[R,F,1480195,5.657932299417004E10,1000.0,4997.424184651347,645781]
[N,F,38767,1.4841356872899961E9,999.96,4990.055330564655,30750]
[N,O,2998603,1.1461753710263995E11,1000.0,4999.978290890792,768912]
TPCH Q3 - extendePrice instead of revenue
[33059171,432867.97000000003,1995-02-16T00:00:00.000Z,0]
[20524164,431359.91000000003,1995-03-04T00:00:00.000Z,0]
[1083941,415404.73,1995-02-21T00:00:00.000Z,0]
[16341859,409805.29,1995-02-25T00:00:00.000Z,0]
[31374434,409371.3,1995-02-17T00:00:00.000Z,0]
[34405031,402898.7,1995-03-09T00:00:00.000Z,0]
[56594855,400094.6,1995-03-13T00:00:00.000Z,0]
[2452422,394861.0,1995-02-22T00:00:00.000Z,0]
[32884775,391267.39,1995-02-18T00:00:00.000Z,0]
[26900320,390404.95999999996,1995-03-12T00:00:00.000Z,0]
TPCH Q5 - extendePrice instead of revenue
[INDIA,1.4134655952000005E9]
[JAPAN,1.4034246889200006E9]
[VIETNAM,1.3858049337799993E9]
[CHINA,1.3806778567499995E9]
[INDONESIA,1.3690848978599997E9]
TPCH Q7 - price instead of revenue
[FRANCE,GERMANY,1992,4.873365616E7]
[FRANCE,GERMANY,1993,5.3386908010000005E7]
[FRANCE,GERMANY,1994,5.6118161500000015E7]
[FRANCE,GERMANY,1995,5.6177264720000006E7]
[FRANCE,GERMANY,1996,5.411796232E7]
[FRANCE,GERMANY,1997,5.697326894E7]
[FRANCE,GERMANY,1998,4.195966787E7]
[GERMANY,FRANCE,1992,4.596645559E7]
[GERMANY,FRANCE,1993,5.7467286599999994E7]
[GERMANY,FRANCE,1994,5.9568380260000005E7]
[GERMANY,FRANCE,1995,5.574328572E7]
[GERMANY,FRANCE,1996,5.817071533000001E7]
[GERMANY,FRANCE,1997,5.685662362000001E7]
[GERMANY,FRANCE,1998,4.0363215349999994E7]
TPCH Q8 - extendePrice instead of market share
[1995,4.372841152E7]
[1996,4.717556235999999E7]
TPCH Q10 - extendePrice instead of revenue, no group by on acctBal (first 20 rows)
[Customer#001485241,UNITED KINGDOM,Fq07MLElZBC54DXzVq9YbP2WP,t,33-116-170-5647,al accounts cajole slyly. ironic, fina,698970.45]
[Customer#001486201,JORDAN,TJhgKueFwrrtXLtenhlw sC2N,23-611-797-4750,ag. deposits along the blithely express instructions w,698431.71]
[Customer#000663001,RUSSIA,18LlI2l6hGpiVkn,32-998-943-3573,ests nag above the accounts. careful, regular sentiments affix. furiously spec,690467.0]
[Customer#000361891,SAUDI ARABIA,wKPyClaA8FXzVmOE7OH68Cn ujxyP,30-105-377-1699,y unusual foxes against the deposits affix slyl,680883.32]
[Customer#000846871,CANADA,qBgCdbbiT0dPMHXaW3ejfzyDhJlf9I3UdrlvG,13-818-536-4472,r, blithe packages among the bold, ironic pla,677859.38]
[Customer#001477261,ETHIOPIA,Ug4Chh6HgFuFuH4kKuDX,y,15-477-193-2424,ticingly around the furiously unusual foxes. expre,666262.76]
[Customer#000996901,UNITED STATES,vspZ5Sp5c5 Z5vDpHIqYXj lNylKNYdf,Hn,34-278-198-3024,its. quickly regular packages sleep doggedly along t,650640.94]
[Customer#000109531,MOROCCO,ctw,V3Lg WsnSF,25-806-287-6640, frets. special packages sleep quickly carefully unusual accounts. carefully final accounts cajole carefully.,645585.34]
[Customer#000116371,MOROCCO,MnsTThR yJf3 VUGbdh2g7Ls,25-461-687-3461,courts nag quickly across the fluffily bold pinto beans. ideas among the furiously regu,645083.1599999999]
[Customer#000478261,JORDAN,nleur50a6uwrpHy5M1aUI6YlTJ3GxdvYr,23-344-728-8021, closely among the blithely even ideas. carefully regular re,639697.71]
[Customer#000822841,KENYA,YegKDa24ghUHejhD9GUgL6GNpToTlaKD8bTBZ,24-127-277-1726, the regular, ironic packages. silent Tiresias wake along the f,638734.97]
[Customer#000596851,ARGENTINA,a1DTx1D4ltckAM8,11-470-165-2441,es along the furiously even requests sleep carefully against the final, pending foxes. regular pinto ,637459.6]
[Customer#000225091,IRAN,ESxBAyRn8EwDJKlPkc,DvPHYFsa85MatFyUzscWY,20-157-662-6929,ic, regular ideas sleep. fluffily final accounts under the blithely ironic requests,628848.14]
[Customer#001080691,UNITED KINGDOM,TbiBgAVDMQhNHKZOb4qwZUN0tIYhTWGDwQTdym,33-375-446-6539,te blithely. carefully express theodolites cajole slyly slyly pending sentiments. blithely special ide,623031.18]
[Customer#001160881,RUSSIA,ACbaUek4MwaAm QpcQAtEN7PUjw3FBBElZIThrC,32-316-891-6777,odolites cajole regular sentiments-- ironic foxes nag express, regular deposits. furiously bold foxes in,614652.5800000001]
[Customer#000001711,MOROCCO,Mhg8c9IAFb8G,25-302-946-6337,gle carefully. final, even deposi,614484.8200000001]
[Customer#001474861,VIETNAM,6cg7FtblHmXnMIjqK11pT47Lsx2,31-595-929-5136,ideas sleep furiously special ,607778.94]
[Customer#001324291,IRAQ,wajEKFPCC6A8Maf450IkC,21-210-951-6699,le furiously blithely unusual excuses. fu,606138.05]
[Customer#000977191,GERMANY,o5XL tB NK8AGE95AuOwL0oz,17-184-695-3349,ully furiously unusual deposits. unusual dolphins sleep according to the even packages. slyly e,597378.05]
[Customer#000850441,ROMANIA,Bq7O5tRkwNHqA37,z1nZ2Ngrg,29-645-452-6044,ickly even theodolites. regular deposits about the care,596343.15]
#+end_example

** Druid Query Results
#+begin_example
Basic Aggregation
[A,F,1478160,5.6539271168E10,1000.0,4997.955256535151,618896]
[N,F,38767,1.48413568E9,999.9600219726562,4990.055330564655,31436]
[N,O,2998581,1.14616795136E11,1000.0,4999.969491236021,754442]
[R,F,1480195,5.657932288E10,1000.0,4997.424184651347,642387]
Ship Date Range
[N,O,1599288]
SubQuery + nation,Type predicates + ShipDate Range
[FRANCE,16,687985.3671875,933.5,5169.0,16]
[GERMANY,19,753095.0,994.0800170898438,5400.421052631579,19]
TPCH Q1
[A,F,1478160,5.6539268096E10,1000.0,4997.955256535151,618896]
[N,F,38767,1.48413568E9,999.9600219726562,4990.055330564655,31436]
[N,O,2998581,1.14616793088E11,1000.0,4999.969491236021,754442]
[R,F,1480195,5.6579323904E10,1000.0,4997.424184651347,642387]
TPCH Q3 - extendePrice instead of revenue
[33059171,432867.9765625,1995-02-16T00:00:00.000Z,0]
[20524164,431359.8984375,1995-03-04T00:00:00.000Z,0]
[1083941,415404.73828125,1995-02-21T00:00:00.000Z,0]
[16341859,409805.3046875,1995-02-25T00:00:00.000Z,0]
[31374434,409371.296875,1995-02-17T00:00:00.000Z,0]
[34405031,402898.6953125,1995-03-09T00:00:00.000Z,0]
[56594855,400094.60546875,1995-03-13T00:00:00.000Z,0]
[2452422,394860.984375,1995-02-22T00:00:00.000Z,0]
[32884775,391267.39453125,1995-02-18T00:00:00.000Z,0]
[26900320,390404.97265625,1995-03-12T00:00:00.000Z,0]
TPCH Q5 - extendePrice instead of revenue
[INDIA,1.413465584E9]
[JAPAN,1.403424672E9]
[VIETNAM,1.385804928E9]
[CHINA,1.380677856E9]
[INDONESIA,1.369084912E9]
TPCH Q7 - price instead of revenue
[FRANCE,GERMANY,1992,4.87336565E7]
[FRANCE,GERMANY,1993,5.3386908E7]
[FRANCE,GERMANY,1994,5.6118162E7]
[FRANCE,GERMANY,1995,5.61772655E7]
[FRANCE,GERMANY,1996,5.4117962E7]
[FRANCE,GERMANY,1997,5.6973269E7]
[FRANCE,GERMANY,1998,4.1959668E7]
[GERMANY,FRANCE,1992,4.5966456E7]
[GERMANY,FRANCE,1993,5.7467286E7]
[GERMANY,FRANCE,1994,5.9568382E7]
[GERMANY,FRANCE,1995,5.5743286E7]
[GERMANY,FRANCE,1996,5.8170716E7]
[GERMANY,FRANCE,1997,5.6856623E7]
[GERMANY,FRANCE,1998,4.0363216E7]
TPCH Q8 - extendePrice instead of market share
[1995,4.3728411E7]
[1996,4.71755625E7]
TPCH Q10 - extendePrice instead of revenue, no group by on acctBal (first 20 rows)
[Customer#001485241,UNITED KINGDOM,Fq07MLElZBC54DXzVq9YbP2WP,t,33-116-170-5647,al accounts cajole slyly. ironic, fina,698970.4375]
[Customer#001486201,JORDAN,TJhgKueFwrrtXLtenhlw sC2N,23-611-797-4750,ag. deposits along the blithely express instructions w,698431.703125]
[Customer#000663001,RUSSIA,18LlI2l6hGpiVkn,32-998-943-3573,ests nag above the accounts. careful, regular sentiments affix. furiously spec,690467.0]
[Customer#000361891,SAUDI ARABIA,wKPyClaA8FXzVmOE7OH68Cn ujxyP,30-105-377-1699,y unusual foxes against the deposits affix slyl,680883.32421875]
[Customer#000846871,CANADA,qBgCdbbiT0dPMHXaW3ejfzyDhJlf9I3UdrlvG,13-818-536-4472,r, blithe packages among the bold, ironic pla,677859.375]
[Customer#001477261,ETHIOPIA,Ug4Chh6HgFuFuH4kKuDX,y,15-477-193-2424,ticingly around the furiously unusual foxes. expre,666262.7734375]
[Customer#000996901,UNITED STATES,vspZ5Sp5c5 Z5vDpHIqYXj lNylKNYdf,Hn,34-278-198-3024,its. quickly regular packages sleep doggedly along t,650640.9375]
[Customer#000109531,MOROCCO,ctw,V3Lg WsnSF,25-806-287-6640," frets. special packages sleep quickly carefully unusual accounts. carefully final accounts cajole carefully.",645585.349609375]
[Customer#000116371,MOROCCO,MnsTThR yJf3 VUGbdh2g7Ls,25-461-687-3461,courts nag quickly across the fluffily bold pinto beans. ideas among the furiously regu,645083.1875]
[Customer#000478261,JORDAN,nleur50a6uwrpHy5M1aUI6YlTJ3GxdvYr,23-344-728-8021," closely among the blithely even ideas. carefully regular re",639697.71875]
[Customer#000822841,KENYA,YegKDa24ghUHejhD9GUgL6GNpToTlaKD8bTBZ,24-127-277-1726," the regular, ironic packages. silent Tiresias wake along the f",638734.9375]
[Customer#000596851,ARGENTINA,a1DTx1D4ltckAM8,11-470-165-2441,"es along the furiously even requests sleep carefully against the final, pending foxes. regular pinto ",637459.576171875]
[Customer#000225091,IRAN,ESxBAyRn8EwDJKlPkc,DvPHYFsa85MatFyUzscWY,20-157-662-6929,ic, regular ideas sleep. fluffily final accounts under the blithely ironic requests,628848.1328125]
[Customer#001080691,UNITED KINGDOM,TbiBgAVDMQhNHKZOb4qwZUN0tIYhTWGDwQTdym,33-375-446-6539,te blithely. carefully express theodolites cajole slyly slyly pending sentiments. blithely special ide,623031.166015625]
[Customer#001160881,RUSSIA,ACbaUek4MwaAm QpcQAtEN7PUjw3FBBElZIThrC,32-316-891-6777,odolites cajole regular sentiments-- ironic foxes nag express, regular deposits. furiously bold foxes in,614652.5703125]
[Customer#000001711,MOROCCO,Mhg8c9IAFb8G,25-302-946-6337,gle carefully. final, even deposi,614484.82421875]
[Customer#001474861,VIETNAM,6cg7FtblHmXnMIjqK11pT47Lsx2,31-595-929-5136,"ideas sleep furiously special ",607778.9375]
[Customer#001324291,IRAQ,wajEKFPCC6A8Maf450IkC,21-210-951-6699,le furiously blithely unusual excuses. fu,606138.0625]
[Customer#000977191,GERMANY,o5XL tB NK8AGE95AuOwL0oz,17-184-695-3349,ully furiously unusual deposits. unusual dolphins sleep according to the even packages. slyly e,597378.06640625]
[Customer#000850441,ROMANIA,Bq7O5tRkwNHqA37,z1nZ2Ngrg,29-645-452-6044,ickly even theodolites. regular deposits about the care,596343.1484375]
#+end_example
